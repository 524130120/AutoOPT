

===== PAGE 1 =====
AutoOptLib Documentation November 2025
several methods for achieving the solutions, such as InputAlg() (preprocessing and inputting
the designed algorithm) and RunAlg() (running the algorithm on the target problem).
2.2.3 O PERATING SEQUENCE
AutoOptLib’s sequence diagram is depicted in Figure 4. To begin with, the interface function Au-
toOpt.m invokes DESIGN.m to instantiate objects (the designed algorithms) of the DESIGN class.
In detail, firstly, DESIGN.m uses the Initialize() method to initialize algorithms over the de-
sign space. Then, the algorithms’ performance on solving the “training” instances1of the target
problem is evaluated by the Evaluate() method. To get the performance, the Evaluate()
method invokes the SOLVE class, and SOLVE further calls functions of the algorithms’ components
and function of the target problem. Finally, the initial algorithms are returned to AutoOpt.m .
AutoOpt.m
Iterate untill terminationDESIGN.m
Return ini tial algori thmsInitializationSOLVE.m Components
Call componentsInitialize
algori thms
Evaluate ini tial algori thms'
performance
Return I nitial algori thms'
 performance
Select algori thms f rom
the curr ent and new ones
Return selected algori thmsProduce  new
algori thms based on
the curr ent onesProblem
  Call training pr oblem instances
Return new algori thmsProduce new algori thms
Call componentsEvaluate new algori thms'
performance
Return new algori thms'
 performance  Call training pr oblem instances
Return final algori thmsTest final algori thms
Call componentsEvaluate final algori thms'
performance
Return final algori thms'
 performance     Cal l test pr oblem instances
Figure 4: Sequence diagram of AutoOptLib.
After initialization, AutoOptLib goes into iterative design. In each iteration, firstly, AutoOpt.m
invokes DESIGN.m . Then, DESIGN.m instantiate new objects (new algorithms) of the DESIGN
class based on the current ones by the Disturb() method. Next, the new algorithms’ performance
1Since the distribution of instances of a real problem is often unknown, one has to sample some of the
problem instances and target these instances (training instances) during the algorithm design procedure. To
avoid the designed algorithms overfit on the training instances, some other instances (test instances) of the
target problem are then employed to test the final algorithms after the design procedure terminates.
6

===== PAGE 2 =====
AutoOptLib Documentation November 2025
is evaluated in the same scheme as in the initialization. After that, the new algorithms are returned
toAutoOpt.m . Finally, the Select() method of the DESIGN class is invoked to select promising
algorithms from the current and new ones.
After the iteration terminates, AutoOpt.m invokes the Evaluate() method of the DESIGN class
to test the final algorithms’ performance on the test instances of the target problem. Then, the final
algorithms are returned in AutoOpt.m .
The above operating sequence has some significant advantages:
1. Metaheuristic component independence - Functions of algorithm components do not in-
teract with each other but invoke independently by the SOLVE class. This independence
provides great flexibility in designing various algorithms and extensibility to new compo-
nents.
2. Design technique packaging - The design techniques are packaged in different methods
(e.g., Disturb() ,Evaluate() ) of the DESIGN class. Such packaging brings good
understandability and openness to new techniques without modifying the library’s archi-
tecture.
3. Target problem separation - The targeted problem is enclosed separately and do not directly
interact with algorithm components and design techniques. This separation allows users to
easily interface their problems with the library and use the library without much knowledge
of metaheuristics and design techniques, thereby ensuring the accessibility of the library to
researchers and practitioners from different communities.
2.3 U SEAUTOOPTLIB
Following the three steps below to use AutoOptLib:
2.3.1 S TEP1: I MPLEMENT PROBLEM
AutoOptLib supports implementing the target problem in Matlab and Python. More formats will be
supported in future versions.
Implement in Matlab:
Users can implement their target optimization problem according to the template prob template.m
in the /Problems folder. prob template.m has three cases. Case "construct" is for setting
problem properties and loading the input data. In particular, line 7 defines the problem type, e.g.,
Problem.type = {"continuous","static","certain"} refers to a continuous static
problem without uncertainty in the objective function. Lines 10 and 11 define the lower and upper
bounds of the solution space. Lines 18 and 21 offer specific settings as indicated in the comments
of lines 14-17 and 20, respectively. Line 25 or 26 is for loading the input data. As a result, problem
proprieties and data are saved in the Problem andData structs, respectively.
1 c a s e ' c o n s t r u c t ' % d e f i n e problem p r o p e r t i e s
2 Problem = v a r a r g i n {1};
3 % d e f i n e problem t y p e i n t h e f o l l o w i n g t h r e e c e l l s .
4 % f i r s t c e l l : ' c o n t i n u o u s ' \' d i s c r e t e ' \' p e r m u t a t i o n '
5 % second c e l l : ' s t a t i c ' \' s e q u e n t i a l '
6 % t h i r d c e l l : ' c e r t a i n ' \' u n c e r t a i n '
7 Problem . t y p e = {' ' , ' ' , ' ' };
8
9 % d e f i n e t h e bound of s o l u t i o n s p a c e
10 lower = [ ] ; % 1 *D, lower bound of t h e D− dimension d e c i s i o n s p a c e
11 upper = [ ] ; % 1 *D, upper bound of t h e D− dimension d e c i s i o n s p a c e
12 Problem . bound = [ lower ; upper ] ;
13
14 % d e f i n e s p e c i f i c s e t t i n g s ( o p t i o n a l ) , o p t i o n s :
15 % ' d e c d i f f ' : e l e m e n t s of t h e s o l u t i o n s h o u l d be d i f f e r e n t w
. r . t each o t h e r f o r d i s c r e t e problems
16 % ' u n c e r t a i n a v e r a g e ' : a v e r a g i n g t h e f i t n e s s o ve r m u l t i p l e f i t n e s s
e v a l u a t i o n s f o r u n c e r t a i n problems
7

===== PAGE 3 =====
AutoOptLib Documentation November 2025
17 % ' u n c e r t a i n w o r s t ' : use t h e worse f i t n e s s among m u l t i p l e f i t n e s s
e v a l u a t i o n s as t h e f i t n e s s f o r u n c e r t a i n problems
18 Problem . s e t t i n g = {' '}; % p u t c h o i c e ( s ) i n t o t h e c e l l
19
20 % s e t t h e number of samples f o r u n c e r t a i n problems ( o p t i o n a l )
21 Problem . sampleN = [ ] ;
22 o u t p u t 1 = Problem ;
23
24 % l o a d / c o n s t r u c t d a t a f i l e i n t h e f o l l o w i n g
25 Data = l o a d ( ' ' ) ; % f o r . mat f o r m a t
26 % Data = r e a d m a t r i x ( ' ' ) ; % f o r o t h e r f o r m a t s
27 o u t p u t 2 = Data ;
Case "repair" is for repairing solutions to keep them feasible, e.g., keeping the solutions within
the box constraint. Lines 2 and 3 input the problem data and solutions (decision variables). Programs
for repairing solutions should be written from line 5. Finally, the repaired solutions will be returned.
1 c a s e ' r e p a i r ' % r e p a i r s o l u t i o n s
2 Data = v a r a r g i n {1};
3 Decs = v a r a r g i n {2};
4 % d e f i n e methods f o r r e p a i r i n g s o l u t i o n s i n t h e f o l l o w i n g
5
6 o u t p u t 1 = Decs ;
Case "evaluate" is for evaluating solutions’ fitness (objective values penalized by constraint
violations). In detail, lines 2 and 3 input the problem data and solutions. The target problem’s
objective function should be written from line 6. Constraint functions (if any) should be written from
line 8. For the constrained problems, AutoOptLib follows the common practice of the metaheuristic
community, i.e., using constraint violations as penalties to discount infeasible solutions. Constraint
violation can be calculated in line 10 by Jain & Deb (2013):
CV(x) =JX
j=1⟨gj(x)⟩+KX
k=1|hk(x)|,
where CV(x)is the constraint violation of solution x;gj(x)andhk(x)are the jth normalized in-
equality constraint and kth normalized equality constraint, respectively, in which the normalization
can be done by dividing the constraint functions by the constant in this constraint present (i.e., for
gj(x)≥bj, the normalized constraint function becomes gj(x) =gj(x)/bj≥0, and similarly
hk(x)can be normalized equality constraint); the bracket operator ⟨gj(x)⟩returns the negative of
gj(x), ifgj(x)<0and returns zeros, otherwise. During solution evaluation, accessory (intermedi-
ate) data for understanding the solutions may be produced. This can be written from line 12. Finally,
the objective values, constraint violations, and accessory data will be returned by lines 13-15.
1 c a s e ' e v a l u a t e ' % e v a l u a t e s o l u t i o n ' s f i t n e s s
2 Data = v a r a r g i n {1}; % l o a d problem d a t a
3 Decs = v a r a r g i n {2}; % l o a d t h e c u r r e n t s o l u t i o n ( s )
4
5 % d e f i n e t h e o b j e c t i v e f u n c t i o n i n t h e f o l l o w i n g
6
7 % d e f i n e t h e i n e q u a l c o n s t r a i n t ( s ) i n t h e f o l l o w i n g , e q u a l
c o n s t r a i n t s s h o u l d be t r a n s f o r m e d t o i n e q u a l ones
8
9 % c a l c u l a t e t h e c o n s t r a i n t v i o l a t i o n i n t h e f o l l o w i n g
10
11 % c o l l e c t a c c e s s o r y d a t a f o r u n d e r s t a n d i n g t h e s o l u t i o n s i n t h e
f o l l o w i n g ( o p t i o n a l )
12
13 o u t p u t 1 = ; % m a t r i x f o r s a v i n g o b j e c t i v e f u n c t i o n v a l u e s
14 o u t p u t 2 = ; % m a t r i x f o r s a v i n g c o n s t r a i n t v i o l a t i o n v a l u e s ( o p t i o n a l
)
15 o u t p u t 3 = ; % m a t r i x or c e l l s f o r s a v i n g a c c e s s o r y d a t a ( o p t i o n a l ) , a
s o l u t i o n ' s a c c e s s o r y d a t a s h o u l d be saved i n a row
8

===== PAGE 4 =====
AutoOptLib Documentation November 2025
Examples of problem implementation can be seen in the CEC 2005 benchmark problem files in
the/Problems/CEC2005 Benchmarks folder. The implementation of a real constrained problem
beamforming.m is given in the /Problems/Real-World/Beanforming folder.
Implement in Python:
The Python port expects a callable with the same three-mode interface used by Matlab problems.
The callable receives a list of problem descriptors, a list of instance indices (or decision vectors when
evaluating), and a mode string. A minimal example is shown below; save it as myproblem.py
and import the callable and pass it directly in Python runs.
1import numpy as np
2def sphere_problem(problems, instances, mode):
3mode = str(mode).lower()
4
5ifmode == "construct":
6 for prob, dim inzip(problems, instances):
7 d = int(dim)
8 prob.type = ["continuous", "static", "certain"]
9 prob.bound = np.array([[-5.0] *d, [5.0] *d], dtype=float)
10
11 def _evaluate(_data, dec):
12 decs = np.asarray(dec, dtype=float)
13 obj = np.sum(decs **2, axis=1, keepdims=True)
14 con = np.zeros_like(obj)
15 return obj, con, None
16
17 prob.evaluate = _evaluate
18 data = [None for _ininstances]
19 return problems, data, None
20
21ifmode == "repair":
22 return instances, None, None
23
24ifmode == "evaluate":
25 decs = np.asarray(instances, dtype=float)
26 obj = np.sum(decs **2, axis=1, keepdims=True)
27 con = np.zeros_like(obj)
28 return obj, con, None
29
30raise ValueError (f"Unsupported mode: {mode }")
This callable can be passed directly (Problem=sphere problem) when using the Python API.
2.3.2 S TEP2: D EFINE DESIGN SPACE
AutoOptLib provides over 40 widely-used algorithm components for designing algorithms for con-
tinuous, discrete, and permutation problems. Each component is packaged in an independent .mfile
in the /Components folder. The included components are listed in Table 1.
The default design space for each type of problems covers all the involved components for this
type, as shown in Listing 1. Users can either employ the default space with the furthest potential to
discover novelty or define a narrow space in Space.m in the /Utilities folder according to interest.
For example, when designing algorithms for continuous problems, the candidate Search components
can be set by collecting the string of component file name in line 4. More components can be added,
which will be detailed in Section 4.1.
Code Listing 1: Design space
1 s w i t c h Problem ( 1 ) . t y p e {1}
2 c a s e ' c o n t i n u o u s '
3 Choose = {' c h o o s e t r a v e r s e ' ; ' c h o o s e t o u r n a m e n t ' ; '
c h o o s e r o u l e t t e w h e e l ' ; ' c h o o s e b r a i n s t o r m ' ; ' c h o o s e n i c h ' };
4 S e a r c h = {' s e a r c h p s o ' ; ' s e a r c h d ec u r r e n t ' ; '
s e a r c h d ec u r r e n t b e s t ' ; ' s e a r c h d er a n d o m ' ; ' c r o s s a r i t h m e t i c '
9

===== PAGE 5 =====
AutoOptLib Documentation November 2025
; ' c r o s s s i m b i n a r y ' ; ' c r o s s p o i n t o n e ' ; ' c r o s s p o i n t t w o ' ; '
c r o s s p o i n t n ' ; ' c r o s s p o i n t u n i f o r m ' ; ' s e a r c h m u g a u s s i a n ' ; '
s e a r c h m u c a u c h y ' ; ' s e a r c h m u p o l y n o m i a l ' ; ' s e a r c h m u u n i f o r m ' ;
' s e a r c h e d a ' ; ' s e a r c h c m a ' ; r e i n i t c o n t i n u o u s ' };
5 Update = {' u p d a t e g r e e d y ' ; ' u p d a t e r o u n d r o b i n ' ; ' u p d a t e p a i r w i s e '
; ' u p d a t e a l w a y s ' ; ' u p d a t e s i m u l a t e d a n n e a l i n g ' };
6
7 c a s e ' d i s c r e t e '
8 Choose = {' c h o o s e t r a v e r s e ' ; ' c h o o s e t o u r n a m e n t ' ; '
c h o o s e r o u l e t t e w h e e l ' ; ' c h o o s e n i c h ' };
9 S e a r c h = {' c r o s s p o i n t o n e ' ; ' c r o s s p o i n t t w o ' ; '
c r o s s p o i n t u n i f o r m ' ; ' c r o s s p o i n t n ' ; ' s e a r c h r e s e t o n e ' ; '
s e a r c h r e s e t r a n d ' ; ' r e i n i t d i s c r e t e ' };
10 Update = {' u p d a t e g r e e d y ' ; ' u p d a t e r o u n d r o b i n ' ; ' u p d a t e p a i r w i s e '
; ' u p d a t e a l w a y s ' ; ' u p d a t e s i m u l a t e d a n n e a l i n g ' };
11
12 c a s e ' p e r m u t a t i o n '
13 Choose = {' c h o o s e t r a v e r s e ' ; ' c h o o s e t o u r n a m e n t ' ; '
c h o o s e r o u l e t t e w h e e l ' ; ' c h o o s e n i c h ' };
14 S e a r c h = {' c r o s s o r d e r t w o ' ; ' c r o s s o r d e r n ' ; ' s e a r c h s w a p ' ; '
s e a r c h s w a p m u l t i ' ; ' s e a r c h s c r a m b l e ' ; ' s e a r c h i n s e r t ' ; '
r e i n i t p e r m u t a t i o n ' };
15 Update = {' u p d a t e g r e e d y ' ; ' u p d a t e r o u n d r o b i n ' ; ' u p d a t e p a i r w i s e '
; ' u p d a t e a l w a y s ' ; ' u p d a t e s i m u l a t e d a n n e a l i n g ' };
16 end
The same operator pools are used in the Python port, where the default space is generated by
autooptlib.utils.space.space ; customise it by modifying the Setting namespace be-
fore invoking the design loop.
2.3.3 S TEP3: R UNAUTOOPTLIB
Users can run AutoOptLib by Matlab command or GUI.
Run by Command:
Users can run AutoOptLib by the following Matlab command:
AutoOpt("name1",value1,"name2",value2,...) ,
where name andvalue refer to the input parameter’s name and value, respectively. The param-
eters are introduced in Table 4. In particular, parameters Metric andEvaluate define the
design objective and algorithm performance evaluation method, respectively. They are summa-
rized in Tables 2 and 3, respectively. The Python port exposes the same parameters through the
autoopt( **kwargs) function; see Section 3 for a runnable script.
Parameters Problem ,InstanceTrain ,InstanceTest , and Mode are mandatory to input
into the command. For other parameters, users can either use their default values without input
to the command or input by themselves for sophisticated functionality. The default param-
eter values can be seen in AutoOpt.m . As an example, AutoOpt("Mode", "design",
"Problem", "beamforming", "InstanceTrain", [1,2], "InstanceTest",
3, "Metric", "quality" is for designing algorithms with the best solution quality on the
beamforming problem.
There are also conditional parameters when certain options of the main parameters are chosen.
For example, setting Metric toruntimeFE incurs conditional parameter Thres to define the
algorithm performance threshold for counting the runtime. All conditional parameters have default
values and are unnecessary to set in the command.
After AutoOptLib running terminates, results will be saved as follows:
• If running the design mode,
–The designed algorithms’ graph representations, phenotypes, parameter values, and
performance will be saved as .mat table in the root dictionary. Algorithms in the .mat
10

===== PAGE 6 =====
AutoOptLib Documentation November 2025
Table 4: Parameters in the commands for running AutoOptLib.
Parameter Type Description
Parameters related to target problem:
Problem character string Function of the target problem
InstanceTrain positive integer Indexes of training instances of the target problem
InstanceTest positive integer Indexes of test instances of the target problem
Parameters related to designed algorithms:
Mode character string Run mode. Options: design - design algorithms for the tar-
get problem, solve - solve the target problem by a designed
algorithm or an existing algorithm.
AlgP positive integer Number of search pathways in a designed algorithm
AlgQ positive integer Maximum number of search operators in a search pathway
Archive character string Name of the archive(s) that will be used in the designed algo-
rithms
LSRange [0,1]real number Range of inner parameter values that make the component
perform local search∗.
IncRate [0,1]real number Minimum rate of solutions’ fitness improvement during 3
consecutive iterations
InnerFE positive integer Maximum number of function evaluations for each call of
local search
Parameters controlling design process:
AlgN positive integer Number of algorithms to be designed
AlgRuns positive integer Number of algorithm runs on each problem instance
ProbN positive integer Population size of the designed algorithms on the target prob-
lem instances
ProbFE positive integer Number of fitness evaluations of the designed algorithms on
the target problem instances
Metric character string Metric for evaluating algorithms’ performance (the ob-
jective of design). Options: quality ,runtimeFE ,
runtimeSec ,auc.
Evaluate character string Method for evaluating algorithms’ performance. Options:
exact ,intensification ,racing ,surrogate .
Compare character string Method for comparing the performance of algorithms. Op-
tions: average ,statistic
AlgFE positive integer Maximum number of algorithm evaluations during the design
process (termination condition of the design process)
Tmax positive integer Maximum running time measured by the number of function
evaluations or wall clock time
Thres real number The lowest acceptable performance of the designed algo-
rithms. The performance can be solution quality.
RacingK positive integer Number of instances evaluated before the first round of racing
Surro real number Number of exact performance evaluations when using surro-
gate
Parameters related to solving target problem:
Alg character string Algorithm file name, e.g., Algs
∗: Some search operators have inner parameters to control performing global or local search. For example, a large mutation probability of
the uniform mutation operator indicates a global search, while a small probability indicates a local search over neighborhood region. As
an example, in cases with LSRange = 0.2, the uniform mutation with probability lower than 0.2is regarded as performing local search,
and the probability equals or higher than 0.2performs global search.
table can later be called by the solve mode to apply to solve the target problem or
make experimental comparisons with other algorithms.
–A report of the designed algorithms’ pseudocode and performance will be saved as
.csvand.xlsx tables, respectively. Users can read, analyze, and compare the algorithms
through the report.
–The convergence curve of the design process (algorithms’ performance versus the
iteration of design) will be depicted and saved as .figfigure. Users can visually analyze
the design process and compare different design techniques through the figure.
• If running the solve mode,
11

===== PAGE 7 =====
AutoOptLib Documentation November 2025
–Solutions to the target problem will be saved as .mat,.csv, and .xlsx tables, respec-
tively.
–Convergence curves of the problem-solving process (solution quality versus algorithm
execution) will be plotted in .figfigure.
Run by GUI:
The GUI can be invoked by the command AutoOpt() without inputting parameters. It is shown
in Figure 5. The GUI has three panels, i.e., Design, Solve, and Results:
• The Design panel is for designing algorithms for a target problem. It has two subpanels,
i.e., Input Problem and Set Parameters:
–Users should load the function of their target problem (e.g., prob template.m or
prob from py.m mentioned in Section 2.3.1) and set the indexes of training and test
instances in the Input Problem subpanel.
–Users can set the main and conditional parameters related to the design in the Set
Parameters subpanel. All parameters have default values for non-expert users’ conve-
nience. The objective of design, the method for comparing the designed algorithms,
and the method for evaluating the algorithms can be chosen by the pop-up menus of
the Metric, Compare, and Evaluate fields, respectively.
After setting the problem and parameters, users can start the run by clicking the RUN
bottom.
• When the running starts, warnings and corresponding solutions to incorrect uses (if any)
will be displayed in the text area at the top of the Results panel. The real-time stage and
progress of the run will also be shown in the area. After the run terminates, results will be
saved in the same format as done by running by commands. Results will also be displayed
on the GUI as follows:
–The convergence curve of the design process will be plotted in the axes area of the
Results panel.
–The pseudocode of the best algorithm found during the run will be written in the text
area below the axes, as shown in Figure 5.
–Users can use the pop-up menu at the bottom of the Results panel to export more
results, e.g., other algorithms found during the run, and detailed performance of the
algorithms on different problem instances.
• The Solve panel is for solving the target problem by an algorithm. It follows a similar
scheme to the Design panel. In particular, users can load an algorithm designed by Au-
toOptLib in the Algorithm File field to solve the target problem. Alternatively, users can
choose a classic algorithm as a comparison baseline through the pop-up menu of the Spec-
ify Algorithm field. AutoOptLib now provides 17 classic metaheuristic algorithms in the
menu. After the problem-solving terminates, the convergence curve and best solutions will
be displayed in the axes and table areas of the Results panel, respectively; detailed results
can be exported by the pop-up menu at the bottom.
2.3.4 O THER USES
Beyond the primary use of automatically designing algorithms, AutoOptLib provides functionalities
of hyperparameter configuration, parameter importance analysis, and benchmark comparison.
Hyperparameter Configuration:
In many scenarios, users may have a preferred algorithm and only need to configure endogenous
parameters (hyperparameters). In essence, hyperparameter configuration is equivalent to designing
an algorithm with predefined component composition but unknown endogenous parameter values.
Thus, it is easy to perform hyperparameter configuration in AutoOptLib by fixing the component
composition and leaving the endogenous parameters tunable in the design space. Following the steps
below to conduct hyperparameter configuration:
1. Implement the target problem as illustrated in Section 2.3.1.
12

===== PAGE 8 =====
AutoOptLib Documentation November 2025
Figure 5: GUI of AutoOptLib.
2. Define the design space as illustrated in Section 2.3.1. In particular, the design space should
only involve the components of the preferred algorithm. The components can be existing
ones in the library or user implemented with the same interface as existing ones. Turn
Setting.TunePara totrue inspace.m .
1 S e t t i n g . TunePara = t r u e ; % t r u e / f a l s e , t r u e f o r h y p e r p a r a m e t e r
c o n f i g u r a t i o n
3. Run AutoOptLib as illustrated in Section 2.3.3.
Parameter Importance Analysis:
Users can further conduct parameter importance analysis by leaving only one tunable parameter
in the design space. That is, users can define the design space with one tunable parameter and
fix the component composition and other parameter values. Then, AutoOptLib runs and returns the
algorithms found during the design process. These algorithms only differ in the values of the tunable
parameter. Users can compare the performance of different parameter values and analyze the impact
of parameter changes on the algorithm performance.
Furthermore, by leaving different parameters tunable in different AutoOptLib runs, users can collect
and compare the trends of each parameter’s changes versus algorithm performance, subsequently
getting insight into each parameter’s importance to the algorithm performance.
Benchmark Comparison:
As illustrated in Section 2.2, different design objectives (Figure 2) and techniques (Figure 3) in
AutoOptLib are implemented with the same architecture; more objectives and techniques can be
added by the same interface. This ensures fair and reproducible comparisons among the objectives
or techniques. The comparison can be conducted by assigning different objectives or techniques to
different AutoOptLib runs on the same targeted problem instances.
13

===== PAGE 9 =====
AutoOptLib Documentation November 2025
AutoOptLib can also be used to benchmark comparisons among different algorithms. Users can set
AlgN >1in the running command or GUI, such that AutoOptLib will design multiple algorithms
in a single run. These algorithms are built and evaluated in a uniform manner during the design
process. Such uniformity ensures a fair comparison among the algorithms.
3 E XAMPLE GALLERY
This section gathers runnable scripts for quick experimentation. Each listing can be copied verbatim
into a file and executed as noted.
3.1 M ATLAB : DESIGNING ALGORITHMS FOR BEAMFORMING
Save the following script as examples/beamforming design.m and run it inside Matlab:
1 % Beamforming d e s i g n example wi th AutoOptLib ( Matlab )
2 a d d p a t h ( ” p a t h / t o / AutoOptLib ” ) ;
3
4 AutoOpt ( ” Mode ” , ” d e s i g n ” , ...
5 ” Problem ” , ” beamforming ” , ...
6 ” I n s t a n c e T r a i n ” , [ 1 , 2 , 3 , 4 , 5 ] , ...
7 ” I n s t a n c e T e s t ” , [ 6 , 7 , 8 , 9 , 1 0 ] , ...
8 ” M e t r i c ” , ” q u a l i t y ” , ...
9 ”AlgN ” , 3 , ...
10 ” AlgFE ” , 5 0 0 , ...
11 ” a r c h i v e ” , ” a r c h i v e b e s t ” ) ;
The run produces Algs.mat ,Algs perf final algs.csv , and the convergence curve .fig
file.
3.2 P YTHON : M INIMAL DESIGN EXPERIMENT
Create examples/design demo.py with the code below and execute python
examples/design demo.py :
1from autooptlib import autoopt
2
3final_algs, alg_trace = autoopt(
4 Mode="design",
5 Problem="cec2013_f1",
6 InstanceTrain=[10],
7 InstanceTest=[10],
8 AlgN=2,
9 AlgFE=80,
10 ProbN=20,
11 ProbFE=800,
12 Evaluate="exact",
13 Compare="average",
14 Archive=["archive_best"],
15)
16
17print ("Best validation metric:", final_algs[0].ave_perform_all().min())
The Python run emits Algs.pkl ,Algs perf final algs.csv , and related trace files.
3.3 P YTHON : SOLVING WITH A BUILT-INALGORITHM
Save as examples/solve demo.py and run python examples/solve demo.py :
14

===== PAGE 10 =====
AutoOptLib Documentation November 2025
1from autooptlib import autoopt
2
3best_runs, all_runs = autoopt(
4 Mode="solve",
5 Problem="cec2013_f1",
6 InstanceSolve=[10],
7 AlgName="Continuous Random Search",
8 AlgRuns=1,
9 ProbN=30,
10 ProbFE=2000,
11 Metric="quality",
12)
13
14print ("Best fitness:", best_runs[0][0].fit)
Result files include Solutions.pkl andFitness allruns.csv .
15